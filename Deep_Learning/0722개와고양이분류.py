#250722 AM9ì‹œ-CNN ìŒ¤PPT415p (250717ë”¥ëŸ¬ë‹ì¢…í•©_ë°±í˜„ìˆ™)
# í´ë”ì— train, test, validation ë“±ìœ¼ë¡œ ë‚˜ëˆˆë‹¤ìŒì— ê°ìì— í´ë”ì— ë¼ë²¨ì„ ë§Œë“¤ê³ 
#ë°ì´í„° ë„£ì–´ë†“ê³ , ImageDataGenerator ë‚˜ DataSet ì„ í†µí•´ì„œ íŒŒì¼ì„ ì§ì ‘ ì½ì–´ì„œ
#í•™ìŠµí•œë‹¤. ë°ì´í„° ì¦ê°• ImageDataGenerator(ì´ˆì°½ê¸°ë¶€í„°)-í´ë”ë¡œë¶€í„° ì§ì ‘ ì´ë¯¸ì§€ íŒŒì¼
#ì„ ì½ì–´ì„œ ê°ì¢… ì²˜ë¦¬ë¥¼ í•´ì„œ ì›í•˜ëŠ” ë§Œí¼ ë°ì´í„°ë¥¼ ëŠ˜ë ¤ì„œ ê°€ì ¸ì˜¨ë‹¤.
#ì¢€ë” ì •ë°€í•˜ê²Œ ë¹„ìŠ·í•œ ì¼ì„ í•œë‹¤. DataSet| - Tensor 2.xxì´í›„ ì¶”ê°€
#ì´ë¯¸ì§€ => numpyë°°ì—´ë¡œ ë°”ê¿”ì„œ í•™ìŠµ:ë°ì´í„°ê°€ ì¶©ë¶„íˆ ë§ìœ¼ë©´

#ê°œì™€ ê³ ì–‘ì´ ì´ë¯¸ì§€ ë¶„ë¥˜
#ë°ì´í„°ì…‹ ì‘ì„ë•Œ ì´ë¯¸ í•™ìŠµëœ CNNí•˜ê³  ì‘ì—…ì‹œ ì‚¬ìš©ì˜ˆì •
#  í´ë”ìƒì„±í•´ì„œ train, test, validationìš© ì‚¬ì§„ì´ë™(ë…¸ê°€ë‹¤ê°€ ìš°ë¦¬ê°€ í• ì¼,ì¤‘ìš”í•¨)
#1. cats_and_dogs_small > train, test, validation ìƒì„±
#   train>(/)cat ê³ ì–‘ì´ì‚¬ì§„1ì²œì¥, /dog ê°œì‚¬ì§„1ì²œì¥ ì´ë™
#   test>(/) cat ê³ ì–‘ì´ì‚¬ì§„5ë°±ì¥, /dog ê°œì‚¬ì§„5ë°±ì¥ ì´ë™
#   validation/cat ê³ ì–‘ì´ì‚¬ì§„5ë°±ì¥, /dog ê°œì‚¬ì§„5ë°±ì¥ ì´ë™

import keras.utils
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import load_model
from keras import models, layers
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np 
import random 
import PIL.Image as pilimg 
import imghdr
import pandas as pd 
import pickle 
import keras 
import os
import shutil

#ì›ë³¸ë°ì´í„°ì…‹ì´ ìˆëŠ” ìœ„ì¹˜ ê²½ë¡œ
# C:\Users\Admin\Documents\GitHub\python_workspace1N\Data_Analysis_2507\data\cats_and_dogs\train
# í˜„ìœ„ì¹˜:\Data_Analysis_2507\DeepLearning
original_dataset_dir = "../data/cats_and_dogs/train" 

#ì´ë™ìœ„ì¹˜ - ê¸°ë³¸ í´ë”
# C:\Users\Admin\Documents\GitHub\python_workspace1N\Data_Analysis_2507\data\cats_and_dogs_small
base_dir = "../data/cats_and_dogs_small"
train_dir = os.path.join(base_dir, 'train')
test_dir = os.path.join(base_dir, 'test' )
validation_dir = os.path.join(base_dir, 'validation')

#ImageDataGeneratorë‚˜ DataSetì´ë‚˜, ë‘ í´ë”ë³´ê³  ìë™ë¼ë²¨ë§
train_cats_dir = os.path.join(train_dir, 'cats' )
train_dogs_dir = os.path.join(train_dir, 'dogs')
test_cats_dir = os.path.join(test_dir, 'cats' )
test_dogs_dir = os.path.join(test_dir, 'dogs')
validation_cats_dir = os.path.join(validation_dir, 'cats')
validation_dogs_dir = os.path.join(validation_dir, 'dogs')

# ì¼€ë¼ìŠ¤(keras) ëª¨ë¸ ì €ì¥ê³¼ ì˜ˆì¸¡í•˜ê¸°, íˆìŠ¤í† ë¦¬ ì €ì¥ë²•
#í•™ìŠµëª¨ë¸(ë„¤íŠ¸ì›Œí¬)- í•™ìŠµì™„ë£Œí•œ ëª¨ë¸ì €ì¥í›„ ê°€ì ¸ì™€ì„œ ì˜ˆì¸¡
model_save_path_keras = 'cat_and_dogs_model.keras' 
#í™•ì¥ìê°€ .h5 â†’ .kerasë¡œ ë³€ê²½, ì¼€ë¼ìŠ¤ê°€ ì§€ì›
history_filepath = 'cat_and_dogs_history.bin'
#í•™ìŠµì‹œë§ˆë‹¤ ì •í™•ë„, ì†ì‹¤ê°’ìˆì´ ì €ì¥í•´ì„œ ì¤Œ
#ì´ê°’ìì²´ëŠ” ì €ì¥ë¯¸ì§€ì›, ë³´í†µì€ pickl(.pkl)ì‚¬ìš©ì €ì¥
#.history ìì²´ë¡œ ì €ì¥í•˜ë©´ ì—ëŸ¬, history.history(íˆìŠ¤í† ë¦¬ì•ˆ íˆìŠ¤í† ë¦¬)ë¡œ ì €ì¥í•˜ë©´ ì •ìƒ

def ImageCopy():
    #ë””ë ‰í† ë¦¬å…§ íŒŒì¼ê°œìˆ˜ ì•Œì•„ë‚´ê¸°(í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ, ì •ë³´ì„±)
    totalCount = len(os.listdir(original_dataset_dir))
    print("ì „ì²´ê°œìˆ˜", totalCount)

    #ë°˜ë³µì‹¤í–‰ ìœ„í•´ ë””ë ‰í† ë¦¬ ì‚­ì œ(ê¸°ì¡´ì— ìˆë‹¤ë©´)
    if os.path.exists(base_dir):
        shutil.rmtree(base_dir, ignore_errors=True, onerror=None)

    #ë””ë ‰í† ë¦¬ ìƒì„±(ê¸°ë³¸base ë° í•˜ìœ„)
    os.makedirs(base_dir) #baseë¶€í„° ë§Œë“¤ê³  ì‹œì‘
    os.makedirs(train_dir)
    os.makedirs(test_dir)
    os.makedirs(validation_dir)

    os.makedirs(train_cats_dir)
    os.makedirs(train_dogs_dir)
    os.makedirs(test_cats_dir)
    os.makedirs(test_dogs_dir)
    os.makedirs(validation_cats_dir)
    os.makedirs(validation_dogs_dir)

    #íŒŒì¼ì´ë™ ë¡œì§ìˆ˜ì •
    #ê³ ì–‘ì´ ì‚¬ì§„ ë³µì‚¬: ì˜®ê¸¸ íŒŒì¼ëª…ì´ cat0.jpg, cat1.jpg ,,, cat1000.jpg
    fnames = [ f'cat.{i}.jpg' for i in range(1000)] 
    for fname in fnames:   #train(0-999): 1000ì¥
        src = os.path.join(original_dataset_dir, fname)
        dst = os.path.join(train_cats_dir, fname)
        shutil.copyfile(src, dst) #1ê°œì”©ë³µì‚¬ x 1ì²œë²ˆ ë°˜ë³µ
    
    fnames = [ f'cat.{i}.jpg' for i in range(1000, 1500)]
    for fname in fnames: #validation(1000-1499): 500ì¥
        src = os.path.join(original_dataset_dir, fname)
        dst = os.path.join(validation_cats_dir, fname)
        shutil.copyfile(src, dst) #1ê°œì”© ë³µì‚¬
    # ['cat.1000.jpg', 'cat.1001.jpg', ..., 'cat.1498.jpg', 'cat.1499.jpg']

    fnames = [ f'cat.{i}.jpg' for i in range(1500, 2000)]
    for fname in fnames:  #test(1500-1999): 500ì¥
        src = os.path.join(original_dataset_dir, fname)
        dst = os.path.join(test_cats_dir, fname)
        shutil.copyfile(src, dst) #1ê°œì”© ë³µì‚¬

    #ì˜®ê¸¸ íŒŒì¼ëª…ì´ dog0.jpg, dog1.jpg ,,, dog1000.jpg
    fnames = [ f'dog.{i}.jpg' for i in range(1000)]
    for fname in fnames:
        src = os.path.join(original_dataset_dir, fname)
        dst = os.path.join(train_dogs_dir, fname)
        shutil.copyfile(src, dst) #1ê°œì”©ë³µì‚¬ x 1ì²œë²ˆ ë°˜ë³µ
    
    fnames = [ f'dog.{i}.jpg' for i in range(1000, 1500)]
    for fname in fnames:
        src = os.path.join(original_dataset_dir, fname)
        dst = os.path.join(validation_dogs_dir, fname)
        shutil.copyfile(src, dst) #1ê°œì”© ë³µì‚¬
    # ['dog.1000.jpg', 'dog.1001.jpg', ..., 'dog.1498.jpg', 'dog.1499.jpg']

    fnames = [ f'dog.{i}.jpg' for i in range(1500, 2000)]
    for fname in fnames:
        src = os.path.join(original_dataset_dir, fname)
        dst = os.path.join(test_dogs_dir, fname)
        shutil.copyfile(src, dst) #1ê°œì”© ë³µì‚¬
# ImageCopy() #ë³µì‚¬í›„ ì£¼ì„ì²˜ë¦¬
# ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì •ë¦¬: ../data/cats_and_dogs_smallì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´, ìƒˆë¡œ ì‹œì‘í•˜ê¸° ìœ„í•´ ì œê±°ë©ë‹ˆë‹¤.
# ìƒˆ ë””ë ‰í† ë¦¬ ìƒì„±: base_dirê³¼ ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬(train, test, validation ë° í•´ë‹¹ cats, dogs í´ë”)ê°€ ìƒì„±ë©ë‹ˆë‹¤.
# ì´ë¯¸ì§€ ë³µì‚¬:
# data  í´ë”ì´ë™
# â”œâ”€â”€ cats_and_dogs
# â”‚   â””â”€â”€ train
# â”‚       â”œâ”€â”€ cat.0.jpg
# â”‚       â”œâ”€â”€ cat.1.jpg
# â”‚       â””â”€â”€ ...
# â””â”€â”€ cats_and_dogs_small
#     â”œâ”€â”€ train
#     â”‚   â”œâ”€â”€ cats  (ê³ ì–‘ì´ ì´ë¯¸ì§€ 1000ì¥)
#     â”‚   â””â”€â”€ dogs  (ê°œ ì´ë¯¸ì§€ 1000ì¥)
#     â”œâ”€â”€ test
#     â”‚   â”œâ”€â”€ cats  (ê³ ì–‘ì´ ì´ë¯¸ì§€ 500ì¥)
#     â”‚   â””â”€â”€ dogs  (ê°œ ì´ë¯¸ì§€ 500ì¥)
#     â””â”€â”€ validation
#         â”œâ”€â”€ cats  (ê³ ì–‘ì´ ì´ë¯¸ì§€ 500ì¥)
#         â””â”€â”€ dogs  (ê°œ ì´ë¯¸ì§€ 500ì¥)

#DataSet ì‚¬ìš©í•˜ê¸°(ì˜›ë‚ ì‹ì½”ë”©ì´ê¸´í•¨)
def deeplearning():
    #ë°ì´í„°ì¦ê°• íŒŒë¼ë¯¸í„°(ë°ì´í„°ì…‹ ì‘ì„ë•Œ ê³¼ëŒ€ì í•© ë°©ì§€) â†’ ì—í¬í¬ë§ˆë‹¤ ë°ì´í„°ë¥¼ ì¡°ê¸ˆì”© ë³€í˜•í•´ì„œ ë°ì´í„°ì…‹í¬ê¸°ë¥¼ ì¸ìœ„ì ìœ¼ë¡œ ëŠ˜ë¦¼
    data_augmentation = keras.Sequential(
            [
		            layers.RandomFlip("horizontal", input_shape=(180, 180, 3)), #ì´ë¯¸ì§€ë¥¼ ìˆ˜í‰ìœ¼ë¡œ ë¬´ì‘ìœ„ë¡œ ë’¤ì§‘ê¸°.
                    layers.RandomRotation(0.1),   #ì´ë¯¸ì§€ë¥¼ ìµœëŒ€ 2pi ë¼ë””ì•ˆì˜10(36ë„)ê¹Œì§€ ë¬´ì‘ìœ„ë¡œ íšŒì „
                    layers.RandomZoom(0.1)      #ì´ë¯¸ì§€ë¥¼ ìµœëŒ€ 10%ê¹Œì§€ ë¬´ì‘ìœ„ë¡œ í™•ëŒ€í•˜ê±°ë‚˜ ì¶•ì†Œí•©ë‹ˆë‹¤.
                ]
        )
    
    #CNN ëª¨ë¸ ì•„í‚¤í…ì²˜
    model = models.Sequential()
    #ì´ë¯¸ì§€ ìŠ¤ì¼€ì¼ë§
    model.add(layers.Rescaling(1./255))  #í”½ì…€ê°’[0, 255]ë²”ìœ„ì—ì„œ [0, 1]ë¡œ ì •ê·œí™”. ì‹ ê²½ë§ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì „ì²˜ë¦¬ë‹¨ê³„
    model.add(data_augmentation)      #ì •ì˜ëœ ë°ì´í„° ì¦ê°• ë ˆì´ì–´ë¥¼ ì ìš©
    model.add(layers.Conv2D(32, (3,3), activation='relu')) 
    #í•„í„°í¬ê¸°ê°€ 3times3ì¸ 32ê°œí•„í„°ë¥¼ ê°€ì§„ 2Dí•©ì„±ê³±ë ˆì´ì–´. í™œì„±í™”í•¨ìˆ˜ë¡œëŠ” relu(Rectified Linear Unit).
    model.add(layers.MaxPooling2D(2,2)) #í’€ì‚¬ì´ì¦ˆê°€2times2(2x2)ì¸ ìµœëŒ€í’€ë§ë ˆì´ì–´. íŠ¹ì§•ë§µì˜ ê³µê°„ì°¨ì›ì„ ì¤„ì—¬ ëª¨ë¸ì´ ì…ë ¥ì˜ ì‘ì€ë³€í™”ì— ë” ê°•ê±´í•˜ë„ë¡ ë„ì›€
    model.add(layers.Conv2D(32, (3,3), activation='relu'))
    model.add(layers.MaxPooling2D(2,2)) #í•©ì„±ê³± ë° í’€ë§í”„ë¡œì„¸ìŠ¤ë¥¼ ë°˜ë³µí•˜ì—¬ ë„¤íŠ¸ì›Œí¬ê°€ ë” ë³µì¡í•œ íŠ¹ì§•ì„ í•™ìŠµê°€ëŠ¥í•˜ê²Œ í•¨
    model.add(layers.Flatten())  #2DíŠ¹ì§•ë§µì„ 1Dë²¡í„°ë¡œ í‰íƒ„í™”í•´ ì™„ì „ì—°ê²°Denseë ˆì´ì–´ë¥¼ìœ„í•œ ë°ì´í„°ì¤€ë¹„
    model.add(layers.Dropout(0.5)) #í•™ìŠµì¤‘ ê°ì—…ëƒì—ì„œ ì…ë ¥ìœ ë‹›ì˜50%ë¥¼ ë¬´ì‘ìœ„ë¡œ 0ìœ¼ë¡œ ì„¤ì •, ë‰´ëŸ°ê°„ì˜ ë³µì¡í•œ ìƒí˜¸ì ì‘ì„ ì¤„ì—¬ ê³¼ì í•©ë°©ì§€
    model.add(layers.Dense(512, activation='relu')) #512ê°œìœ ë‹›ê³¼ reluí™œì„±í™”ë¥¼ ê°€ì§„ ì™„ì „ì—°ê²°Denseë ˆì´ì–´
    model.add(layers.Dense(1, activation='sigmoid')) # ì¶œë ¥ë ˆì´ì–´,ì´ì§„ë¶„ë¥˜ì´ë¯€ë¡œ sigmoid ì‚¬ìš©

    #ëª¨ë¸ì»´íŒŒì¼
    model.compile(optimizer='adam',  #adamì˜µí‹°ë§ˆì´ì €ëŠ” í•™ìŠµì¤‘ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ”ë° ì¼ë°˜ì ì´ê³  íš¨ê³¼ì ì¸ ì„ íƒ
                  loss="binary_crossentropy", #ì´ì§„ë¶„ë¥˜ë‹ˆ ì´ì§„êµì°¨ì—”íŠ¸ë¡œí”¼ ì†ì‹¤í•¨ìˆ˜, ì˜ˆì¸¡ëœí™•ë¥ ê³¼ ì‹¤ì œ ì´ì§„ë ˆì´ë¸”ê°„ì˜ ì°¨ì´ ì¸¡ì •
                  metrics=['accuracy']) #ëª¨ë¸ì„±ëŠ¥=ì •í™•ë„ ê¸°ì¤€ í‰ê°€
    
    #ë°ì´í„°ì…‹ ë¡œë“œ(í•™ìŠµ) - train_dirí´ë”ë¡œë¶€í„° ì´ë¯¸ì§€íŒŒì¼ ì½ì–´ì˜¤ê¸°
    train_ds = keras.utils.image_dataset_from_directory( 
        train_dir,             #í•™ìŠµì´ë¯¸ì§€ê°€ í¬í•¨ëœ dirì§€ì •
        validation_split=0.2,   #train_dirì˜ í›ˆë ¨ì…‹ì„ í›ˆë ¨ì…‹20:ê²€ì¦ì…‹80%ë¡œ ë‚˜ëˆ  ê²€ì¦
        seed=123,               # seedë¥¼ ì‚¬ìš©í•˜ì—¬ 2:8ë¶„í•  ì¼ê´€ì„± ìœ ì§€
        subset="training",  #ë¶„í• ë¶„ ì¤‘ í•™ìŠµtraining ë˜ëŠ” ê²€ì¦validationë¶€ë¶„ì„ ë¡œë“œì—¬ë¶€ ì§€ì •
        image_size=(180,180), #ì´ë¯¸ì§€í”½ì…€í¬ê¸° ì¡°ì • 180x180
        batch_size=16  #ì´ë¯¸ì§€16ê°œì”© batchë¬¶ìŒìœ¼ë¡œ ë¡œë“œ
    )
    #ì°¸ê³ : í˜„ì¬ ì½”ë“œëŠ” train_dirì—ì„œ ë¶„í• í•˜ì—¬ train_dsì™€ val_dsë¥¼ ëª¨ë‘ ë¡œë“œ. ì´ëŠ” ImageCopy()ì— ì˜í•´ ìƒì„±ëœ validation ë””ë ‰í† ë¦¬ê°€ í•™ìŠµ ì¤‘ ëª¨ë¸ ê²€ì¦ì— ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë¯¸ë¦¬ ë¶„ë¦¬ëœ validation_dirì„ ê²€ì¦ì— ì‚¬ìš©í•˜ë ¤ë©´, validation_split ë° subset ì¸ìˆ˜ ì—†ì´ validation_dirì—ì„œ val_dsë¥¼ ì§ì ‘ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.

    #ë°ì´í„°ì…‹ ë¡œë“œ(ê²€ì¦)
    val_ds = keras.utils.image_dataset_from_directory( 
        train_dir,
        validation_split=0.2,
        seed=123,
        subset="validation",
        image_size=(180,180),
        batch_size=16
    )

    #ëª¨ë¸í•™ìŠµ fitë©”ì†Œë“œ
    history = model.fit(train_ds,  #í•™ìŠµ ë°ì´í„°ì…‹
                        validation_data=val_ds, #í•™ìŠµì¤‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ê³¼ì í•©ì„ ê°ì§€í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ê²€ì¦ë°ì´í„°ì…‹
                        epochs=30) 
    
    #ëª¨ë¸ ì €ì¥í•˜ê¸°
    try:
        model.save(model_save_path_keras) #í•™ìŠµëœ Keras ëª¨ë¸ì€ cat_and_dogs_model.kerasì— ì €ì¥
        print("ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"ëª¨ë¸ ì €ì¥ì¤‘ ì˜¤ë¥˜ ë°œìƒ {e}")

    #íˆìŠ¤í† ë¦¬ ì €ì¥(í•„ìš”ì‹œ)
    try:
        with open(history_filepath, 'wb') as file: #cat_and_dogs_history.binì— ì €ì¥
            pickle.dump(history.history, file)
        print("íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"íˆìŠ¤í† ë¦¬ ì €ì¥ì¤‘ ì˜¤ë¥˜ ë°œìƒ {e}")

    # model.save(model_save_path_keras)#ëª¨ë¸ ì €ì¥í•˜ê¸°
    # model.save("catanddog.keras") #.kerasí™•ì¥ìë¡œ ëª¨ë¸ì €ì¥
    
# deeplearning()

# í•™ìŠµ íˆìŠ¤í† ë¦¬ ì‹œê°í™”
def drawCart():
    print("--- ì €ì¥ëœ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° ---")
    try:
        load_model_keras = keras.models.load_model(model_save_path_keras)
        print("ëª¨ë¸ í˜¸ì¶œ ì„±ê³µ")
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë”©ì¤‘ ì‹¤íŒ¨: {e}")

    print("íˆìŠ¤í† ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°")
    try:
        with open(history_filepath, 'rb') as file:
            history = pickle.load(file)
            print("íˆìŠ¤í† ë¦¬ ë¡œë”© ì„±ê³µ")
    except Exception as e:
        print(f"íˆìŠ¤í† ë¦¬ ë¡œë”©ì¤‘ ì‹¤íŒ¨ : {e}")

    #íˆìŠ¤í† ë¦¬ì˜ í‚¤ê°’ë“¤ ê°€ì ¸ì˜¤ê¸° - ì—í¬í¬íšŸìˆ˜ë§Œí¼ listë¡œ ê°€ì ¸ì˜¨ë‹¤
    acc = history['accuracy']
    val_acc = history['val_accuracy']
    loss = history['loss']
    val_loss = history['val_loss']

    #Xì¶•(ì—í¬í¬) ì¢Œí‘œê°’
    X = range(len(acc))

    plt.plot(X, acc, 'ro', label="Training accuracy")
    plt.plot(X, val_acc, 'b', label='Validation accuracy')
    plt.title("Training and Validation accuracy")
    plt.legend() #ë²”ë¡€í‘œì‹œ(ìš°ì¸¡ê¸°ë³¸)

    plt.figure() #ìƒˆì°½ìœ¼ë¡œ ì°¨íŠ¸ë„ì›€
    plt.plot(X, loss, 'ro', label='Training loss')
    plt.plot(X, val_loss, 'b', label='Validation loss')
    plt.title("Trainig and Validation loss")
    plt.legend() #ë²”ë¡€í‘œì‹œ(ìš°ì¸¡ê¸°ë³¸)
    plt.show() #ëª¨ë‘ì¶œë ¥

#ëª¨ë¸ ì˜ˆì¸¡
def Predict():
    load_model_keras = None     #í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    try:
        load_model_keras = keras.models.load_model(model_save_path_keras)
        print("ëª¨ë¸ í˜¸ì¶œ ì„±ê³µ")
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë”©ì¤‘ ì‹¤íŒ¨: {e}")
        return

    #ì˜ˆì¸¡ ë°ì´í„°ì…‹ ë¡œë“œ
    val_ds = keras.utils.image_dataset_from_directory( 
        train_dir,
        validation_split=0.2,
        seed=123,
        subset="validation",
        image_size=(180,180),
        batch_size=16
    )

    print("--- ìë™ ë¼ë²¨ë§ í™•ì¸í•˜ê¸° ---")
    class_names = val_ds.class_names
    print(class_names) #í´ë”ëª… ê¸°ë°˜ìœ¼ë¡œ ['cats', 'dogs']

    total_match_count = 0 #ì „ì²´ì¼ì¹˜í•œê°œìˆ˜
    total_samples_process = 0 #ì „ì²´ ì²˜ë¦¬ê°œìˆ˜
    max_samples_to_process = 500 #ë°ì´í„°ë¥¼ 500ê°œë§Œ ì˜ˆì¸¡
    #ì˜ˆì¸¡ ë£¨í”„: `val_ds`ì˜ ì´ë¯¸ì§€ ë°°ì¹˜ë“¤ì„ ë°˜ë³µ
    for input_batch, labels_batch in val_ds:
        #val_dsê°€ í´ë”ë¡œë¶€í„° ì´ë¯¸ì§€íŒŒì¼ ì½ì–´ì˜¤ëŠ”ë° batch_sizeë§Œí¼ì”© ì½ì–´ì˜¨ë‹¤
        total_samples_process += len(labels_batch)
    # print(total_samples_process)

        #ì˜ˆì¸¡í•˜ê¸°
        predictions_batch = load_model_keras.predict(input_batch, verbose=2)
        print(predictions_batch)

        #í™•ë¥ ì„ í´ë˜ìŠ¤ ë ˆì´ë¸”ë¡œ ë³€í™˜(ì‹œê·¸ëª¨ì´ë“œì¶œë ¥ì— ë”°ë¼ ê³ ì–‘ì´ 0, ê°œ 1)
        for i in predictions_batch:
            if i>=0.5:
                print("ê°œ")
            else:
                print("ê³ ì–‘ì´")
        # print(predicted_classes)
        #ì„ íƒ: 4
        # ëª¨ë¸ í˜¸ì¶œ ì„±ê³µ
        # Found 2000 files belonging to 2 classes.
        # Using 400 files for validation.
        # --- ìë™ ë¼ë²¨ë§ í™•ì¸í•˜ê¸° ---
        # ['cats', 'dogs']
        # 1/1 - 0s - 453ms/step
        # [[0.5129856 ]    # ê°œ (1ì—ê°€ê¹Œìš°ë©´)
        #  [0.16446646]    # ê³ ì–‘ì´ (0ì— ê°€ê¹Œìš°ë©´)
        #  [0.99132216]    # ê°œ
        #  [0.13068011]    # ê³ ì–‘ì´
        #  [0.9697823 ]    # ê°œ
        #  [0.63931817]    # ê°œ
        #  [0.93372333]    # ê°œ
        #  [0.08725409]    # ê³ ì–‘ì´
        #  [0.88186204]    # ê°œ
        #  [0.75304776]    # ê°œ
        #  [0.862337  ]    # ê°œ
        #  [0.8830812 ]    # ê°œ
        #  [0.607399  ]    # ê°œ
        #  [0.9224868 ]    # ê°œ
        #  [0.5469079 ]    # ê°œ
        #  [0.1390551 ]]    # ê³ ì–‘ì´        

        #ì˜ˆì¸¡ê²°ê³¼ì™€ ì‹¤ì œ ë ˆì´ë¸” ë¹„êµ
        #ì´ì§„ë¶„ë¥˜ë¼ì„œ ê²°ê³¼ê°’ì´ 1ê°œë§Œ ì˜¨ë‹¤. ê½ƒë¶„ë¥˜ëŠ” ì´ë ‡ê²Œ í•´ë„ ë˜ì§€ë§Œ ê³ ì–‘ì´ëŠ” ì•ˆë¨
        #ì´ì§„ë¶„ë¥˜ì‹œ ë¼ë²¨ì´1ì¸ìš”ì†Œì˜ í™•ë¥  ì „ë‹¬
        #ë‹¤ì¤‘ë¶„ë¥˜ì‹œ [0.1, 0.1, 0.6, 0.1, 0.1]
        # pridicted_calsses = np.argmax(predictions_batch, axis=1)
        predicted_calsses = (predictions_batch>0.5).astype(int) #0.5ë³´ë‹¤í¬ë©´T,ì‘ìœ¼ë©´F
        print("ì˜ˆì¸¡: ", predicted_calsses.flatten()) #ì°¨ì›
        print("ë¼ë²¨: ", labels_batch.numpy())   #tensor â†’ numpy
        # break #ì—¬ê¸°ì„œ ì˜ˆì¸¡í•œê²ƒ í™•ì¸í›„ ì£¼ì„ì²˜ë¦¬

        match_count = np.sum(predicted_calsses.flatten() == labels_batch.numpy())
        total_match_count += match_count
     
    # print(total_samples_process, len(labels_batch)) # 16 16
    print("ì „ì²´ ë°ì´í„°ê°œìˆ˜", total_samples_process) # 16
    print("ì •ë‹µ ë°ì´í„°ê°œìˆ˜", total_match_count) # 
    print("ì˜¤ë‹µ ë°ì´í„°ê°œìˆ˜", total_samples_process-total_match_count) 

    # ì„ íƒ: 4
    # ëª¨ë¸ í˜¸ì¶œ ì„±ê³µ
    # Found 2000 files belonging to 2 classes.
    # Using 400 files for validation.
    # --- ìë™ ë¼ë²¨ë§ í™•ì¸í•˜ê¸° ---
    # ['cats', 'dogs']
    # 1/1 - 0s - 210ms/step
    # [[0.5129856 ]
    # ...
    # ì˜ˆì¸¡:  [1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1]
    # ë¼ë²¨:  [1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1]
    # 1/1 - 0s - 96ms/step
    # [[0.38705036]   # ê³ ì–‘ì´
    # [0.78980595]    # ê°œ
    # [0.25163677]    # ê³ ì–‘ì´
    # [0.10577456]    # ê³ ì–‘ì´
    # [0.6678745 ]    # ê°œ
    # [0.99406433]    # ê°œ
    # [0.9999997 ]    # ê°œ
    # [0.7170931 ]    # ê°œ
    # [0.5183464 ]    # ê°œ
    # [0.8482667 ]    # ê°œ
    # [0.06098574]    # ê³ ì–‘ì´
    # [0.36380434]    # ê³ ì–‘ì´
    # [0.6999855 ]    # ê°œ
    # [0.86362666]    # ê°œ
    # [1.        ]    # ê°œ
    # [0.9881058 ]]   # ê°œ
    # ì˜ˆì¸¡:  [0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1]
    # ë¼ë²¨:  [0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1]
    # 2025-07-23 11:42:40.222624: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
    # ì „ì²´ ë°ì´í„°ê°œìˆ˜ 400
    # ì •ë‹µ ë°ì´í„°ê°œìˆ˜ 282
    # ì˜¤ë‹µ ë°ì´í„°ê°œìˆ˜ 118

def main():
    while True:
        print("1. íŒŒì¼ë³µì‚¬")
        print("2. í•™ìŠµ")
        print("3. ì°¨íŠ¸")
        print("4. ì˜ˆì¸¡")
        sel = input("ì„ íƒ: ")
        if sel=="1":
            ImageCopy()
        elif sel=="2":
            deeplearning()
        elif sel=="3":
            drawCart()
        elif sel=="4":
            Predict()
        else:
            break

if __name__=="__main__":
    main()

# (mytensorflow) C:\Users\Admin\Documents\GitHub\python_workspace1N\Data_Analysis_2507\DeepLearning>python 0722ê°œì™€ê³ ì–‘ì´ë¶„ë¥˜.py
# 2025-07-22 10:46:10.528593: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
# 2025-07-22 10:46:13.227044: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
# C:\Users\Admin\Documents\GitHub\python_workspace1N\Data_Analysis_2507\DeepLearning\0722ê°œì™€ê³ ì–‘ì´ë¶„ë¥˜.py:26: DeprecationWarning: 'imghdr' is deprecated and slated for removal in Python 3.13
#   import imghdr
# 2025-07-22 10:46:21.108580: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
# To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
# Found 2000 files belonging to 2 classes.
# Using 1600 files for training.
# Found 2000 files belonging to 2 classes.
# Using 400 files for validation.
# Epoch 1/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 58s 527ms/step - accuracy: 0.9904 - loss: 0.0311 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 2/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 50s 503ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 3/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 53s 521ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 4/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 51s 506ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 5/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 50s 500ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 6/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 50s 497ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 7/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 50s 496ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 8/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 48s 480ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 9/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47s 473ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 10/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 48s 481ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00


# ìµœì¢… 250722
# (mytensorflow) C:\Users\Admin\Documents\GitHub\python_workspace1N\Data_Analysis_2507\DeepLearning>python 0722ê°œì™€ê³ ì–‘ì´ë¶„ë¥˜.py
# 2025-07-22 13:15:23.972770: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
# 2025-07-22 13:15:27.747528: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
# C:\Users\Admin\Documents\GitHub\python_workspace1N\Data_Analysis_2507\DeepLearning\0722ê°œì™€ê³ ì–‘ì´ë¶„ë¥˜.py:26: DeprecationWarning: 'imghdr' is deprecated and slated for removal in Python 3.13
#   import imghdr
# C:\ProgramData\anaconda3\envs\mytensorflow\Lib\site-packages\keras\src\layers\preprocessing\tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
#   super().__init__(**kwargs)
# 2025-07-22 13:15:39.127680: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
# To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
# Found 2000 files belonging to 2 classes.
# Using 1600 files for training.
# Found 2000 files belonging to 2 classes.
# Using 400 files for validation.
# Epoch 1/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 55s 501ms/step - accuracy: 0.9485 - loss: 0.0411 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 2/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 49s 487ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 3/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 49s 487ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 4/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 48s 484ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 5/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 48s 482ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 6/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 49s 494ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 7/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 49s 488ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 8/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 49s 488ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 9/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 49s 486ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 10/10
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 50s 495ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00

# 250723 AM10ì‹œ
# (mytensorflow) C:\Users\Admin\Documents\GitHub\python_workspace1N\Data_Analysis_2507\DeepLearning>python 0722ê°œì™€ê³ ì–‘ì´ë¶„ë¥˜.py                              
# 2025-07-23 09:28:46.383750: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
# 2025-07-23 09:28:50.194956: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
# C:\Users\Admin\Documents\GitHub\python_workspace1N\Data_Analysis_2507\DeepLearning\0722ê°œì™€ê³ ì–‘ì´ë¶„ë¥˜.py:26: DeprecationWarning: 'imghdr' is deprecated and slated for removal in Python 3.13
#   import imghdr
# C:\ProgramData\anaconda3\envs\mytensorflow\Lib\site-packages\keras\src\layers\preprocessing\tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
#   super().__init__(**kwargs)
# 2025-07-23 09:29:01.356244: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
# To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
# Found 2000 files belonging to 2 classes.
# Using 1600 files for training.
# Found 2000 files belonging to 2 classes.
# Using 400 files for validation.
# Epoch 1/30
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 57s 507ms/step - accuracy: 1.0000 - loss: 0.0282 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 2/30
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 50s 504ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# ...
# # Epoch 27/30
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 48s 481ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 28/30
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 48s 483ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.100/100 11100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 48s 483ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 29/30
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 48s 481ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# Epoch 30/30
# 100/100 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 48s 479ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00
# ëª¨ë¸ ì €ì¥ ì™„ë£Œ
# íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ

""" 
# # 53#í•™ìŠµëª¨ë¸(ë„¤íŠ¸ì›Œí¬)- í•™ìŠµì™„ë£Œí•œ ëª¨ë¸ì €ì¥í›„ ê°€ì ¸ì™€ì„œ ì˜ˆì¸¡ -ì‰¬ìš´ì„¤ëª…
# 1. ëª¨ë¸ ì €ì¥ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°
#  â€¢ ëª¨ë¸ ì €ì¥

# í•™ìŠµì´ ëë‚œ ë”¥ëŸ¬ë‹(ì‹ ê²½ë§) ëª¨ë¸ì€ íŒŒì¼ë¡œ ì €ì¥ ê°€ëŠ¥

# ìµœê·¼ì—ëŠ” íŒŒì¼ í™•ì¥ìë¥¼ .kerasë¥¼ ì‚¬ìš©

# # model_save_path_keras = 'cat_and_dogs_model.keras'
# # model.save(model_save_path_keras)
# ê³¼ê±°ì—ëŠ” .h5(HDF5 í˜•ì‹)ê°€ ì£¼ë¡œ ì‚¬ìš©ëì§€ë§Œ, ì§€ê¸ˆì€ ì¼€ë¼ìŠ¤ì—ì„œ .kerasë¥¼ ê¶Œì¥

#  â€¢ ì €ì¥í•œ ëª¨ë¸ ë¶ˆëŸ¬ì™€ì„œ ì˜ˆì¸¡í•˜ê¸°

# ì €ì¥ëœ ëª¨ë¸ì„ ë‹¤ì‹œ ë¶ˆëŸ¬ì™€ì„œ ë°”ë¡œ ì˜ˆì¸¡(ì¶”ë¡ ) ê°€ëŠ¥
# # from keras.models import load_model
# # model = load_model(model_save_path_keras)
# # predictions = model.predict(new_data)

# 2. í•™ìŠµ ê³¼ì • ê¸°ë¡(history) ì €ì¥í•˜ê¸°
#  â€¢ historyë€?

# ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ model.fit()ì„ í˜¸ì¶œí•˜ë©´, í•™ìŠµ ê³¼ì •ì˜ ì •í™•ë„, ì†ì‹¤ ê°’ ë“±ì´ ë‹´ê¸´ history ê°ì²´ê°€ ë°˜í™˜ë¨

#  â€¢ history ê°ì²´ì˜ ì‹¤ì œ ë°ì´í„°

# historyì—ëŠ” ì—¬ëŸ¬ ì •ë³´ê°€ ë“¤ì–´ ìˆì§€ë§Œ, ì‹¤ì œ ì •í™•ë„/ì†ì‹¤ ê°’ ë°ì´í„°ëŠ”
# history.history(ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)ì— ë“¤ì–´ ìˆìŒ

#  â€¢ ë°”ë¡œ ì €ì¥í•˜ë©´ ì˜¤ë¥˜ ë°œìƒ
# history ê°ì²´ ìì²´ë¥¼ ê·¸ëŒ€ë¡œ ì €ì¥í•˜ë©´ ì—ëŸ¬ê°€ ë°œìƒ
# ë°˜ë“œì‹œ history.historyë§Œ ë”°ë¡œ ë½‘ì•„ì„œ ì €ì¥í•´ì•¼ í•¨
# ì˜ˆì‹œ ì½”ë“œ: íˆìŠ¤í† ë¦¬ ì €ì¥ & ë¶ˆëŸ¬ì˜¤ê¸° (pickle ì‚¬ìš©)

# # import pickle

# # ì €ì¥
# # with open('cat_and_dogs_history.bin', 'wb') as f:
# #     pickle.dump(history.history, f)

# # ë¶ˆëŸ¬ì˜¤ê¸°
# # with open('cat_and_dogs_history.bin', 'rb') as f:
# #     loaded_history = pickle.load(f)

# # print(loaded_history['accuracy'])  # ì˜ˆ: í•™ìŠµ ì‹œ epochë³„ ì •í™•ë„ê°’
#  â€¢ pickleë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ ì†ì‰½ê²Œ íŒŒì´ì¬ ë°ì´í„°(ë”•ì…”ë„ˆë¦¬ ë“±)ë¥¼ íŒŒì¼ë¡œ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸° ê°€ëŠ¥

# 3. í•µì‹¬ ì •ë¦¬
#  â€¢ ëª¨ë¸ ì €ì¥: model.save('íŒŒì¼ëª….keras')

#  â€¢ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°: load_model('íŒŒì¼ëª….keras') â†’ ë°”ë¡œ ì˜ˆì¸¡ ê°€ëŠ¥

#  â€¢ history ì €ì¥: ë°˜ë“œì‹œ history.historyë§Œ ì¶”ì¶œí•´ì„œ pickle ë“±ìœ¼ë¡œ ì €ì¥í•´ì•¼ í•¨.
# (ì „ì²´ history ì €ì¥ì€ ì§€ì›ë˜ì§€ ì•Šê³ , ì‹œë„í•˜ë©´ ì—ëŸ¬)

#  â€¢ í™•ì¥ì: ëª¨ë¸ì€ .keras(ë˜ëŠ” ì˜ˆì „ë°©ì‹ .h5), íˆìŠ¤í† ë¦¬ëŠ” .bin/.pkl ë“± ì•„ë¬´ê±°ë‚˜ ê°€ëŠ¥

# ğŸ”‘ í•œ ë¬¸ì¥ ìš”ì•½
# í•™ìŠµí•œ ëª¨ë¸ì€ .kerasë¡œ ì €ì¥ í›„ ë°”ë¡œ ë¶ˆëŸ¬ì™€ ì˜ˆì¸¡í•  ìˆ˜ ìˆê³ ,
# í•™ìŠµ ê³¼ì • ê¸°ë¡(history)ì€ history.historyë¥¼ ë”°ë¡œ pickleë¡œ ì €ì¥í•´ì•¼ ì—ëŸ¬ ì—†ì´ í™œìš©í•  ìˆ˜ ìˆë‹¤.
"""