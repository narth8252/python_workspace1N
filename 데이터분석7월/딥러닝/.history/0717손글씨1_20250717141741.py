# Ctrl + Shift + P â†’ "Python: Select Interpreter" ì…ë ¥í•´ì„œ ì„ íƒ
# C:\ProgramData\Anaconda3\envs\deeplearning\python.exe
#conda activate deeplearning

# MNIST ìˆ«ì ë¶„ë¥˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ íë¦„ ì •ë¦¬
from tensorflow.keras.datasets import mnist
import tensorflow as tf
from tensorflow import keras

tf.random.set_seed(1234)

#1.MNIST ë°ì´í„°ì…‹ ë¡œë“œ (mnist.load_data())
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
print(type(train_images), type(train_labels)) #<class 'numpy.ndarray'> <class 'numpy.ndarray'>
# train_images, train_labels ë‘˜ ë‹¤ numpy.ndarray íƒ€ì…ìœ¼ë¡œ ì˜ ë¶ˆëŸ¬ì™”ë‹¤ëŠ” ëœ»ì´ì•¼.

# 2. ë°ì´í„° ì „ì²˜ë¦¬ (reshape + ì •ê·œí™”,ìŠ¤ì¼€ì¼ë§)
# 3D â†’ 2D ë°°ì—´ë¡œ ë°”ê¾¸ê³ , í”½ì…€ ê°’ 0255 â†’ 01ë¡œ ë°”ê¿¨ì–´.
train_images = train_images.reshape(train_images.shape[0], 28 * 28)
train_images = train_images.astype(float) / 255
test_images = test_images.reshape(test_images.shape[0], 28 * 28)
test_images = test_images.astype(float) / 255

print(train_images.shape, train_images.min(), train_images.max())
# í›ˆë ¨ë°ì´í„°shapeê°€ (60000, 784) 0.0 1.0 (ìµœì†Œê°’0.0, ìµœëŒ€ê°’1.0)ìœ¼ë¡œ ì •ìƒì ì¸ì „ì²˜ë¦¬
# í›ˆë ¨ë°ì´í„°ê°€ 60000ê°œ, ê° ì´ë¯¸ì§€ê°€ 784(28*28) í”½ì…€ë¡œ 1ì°¨ì› ë°°ì—´í™”ëê³ , ê°’ë„ 0~1 ë²”ìœ„ë¡œ ì˜ ì •ê·œí™”ëì–´.
print(test_images.shape, test_images.min(), test_images.max())
# í…ŒìŠ¤íŠ¸ë°ì´í„°ê°€ (10000, 784)í˜•íƒœì´ê³  0.0 1.0 ë¡œ í›ˆë ¨ë°ì´í„°ì™€ ë™ì¼í•˜ê²Œ ì „ì²˜ë¦¬ëì–´.

# 3. ëª¨ë¸ìƒì„± í›„ ì‹¤í–‰
# ì…ë ¥ì¸µ 64ê°œ ë‰´ëŸ°, ì¶œë ¥ì¸µ 10ê°œ ë‰´ëŸ°(ìˆ«ì 0~9 í´ë˜ìŠ¤)
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(28*28,)),
    layers.Dense(10, activation='softmax')
])
# ëª¨ë¸ êµ¬ì¡°ë¥¼ ì¶œë ¥í•´ë³´ì
model.summary() #ëª¨ë¸êµ¬ì¡°ì¶œë ¥
# Model: "sequential"
# _________________________________________________________________
#  Layer (type)                Output Shape              Param # (íŒŒë¼ë¯¸í„°ìˆ˜)
# =================================================================
#  dense (Dense)  1stì…ë ¥ì¸µ       (None, 64)í¸í–¥            50240   #ì²«ë²ˆì§¸Denseì¸µ(ì…ë ¥ì¸µ)íŒŒë¼ë¯¸í„°ìˆ˜: 50,240ê°œ(ì…ë ¥ 784*64+í¸í–¥64)
#  dense_1 (Dense)2ndì¶œë ¥ì¸µ       (None, 10)í¸í–¥             650    #ë‘ë²ˆì§¸Denseì¸µ(ì¶œë ¥ì¸µ)íŒŒë¼ë¯¸í„°ìˆ˜: 650ê°œ(64*10+í¸í–¥10)
# =================================================================
# Total params: 50,890      #ì´íŒŒë¼ë¯¸í„°: 50,890ê°œ 
# Trainable params: 50,890  #ëª¨ë‘ í•™ìŠµê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ì„
# Non-trainable params: 0
# _________________________________________________________________

# 4. ëª¨ë¸ ì»´íŒŒì¼
# â†’ ìˆ«ì ë¼ë²¨ ê·¸ëŒ€ë¡œ ì“¸ ê±°ë‹ˆê¹Œ ì†ì‹¤í•¨ìˆ˜ëŠ” sparse_categorical_crossentropy
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# 5. ëª¨ë¸ í•™ìŠµ ì‹œì‘
# â†’ 64ê°œì”© 5ë²ˆ í•™ìŠµí•˜ê³ , ë§¤ ì—í­ë§ˆë‹¤ ì„ì–´ì„œ í•™ìŠµí•˜ëŠ” ì¤‘
history = model.fit(
    train_images,
    train_labels,
    validation_data=(test_images, test_labels),
    batch_size=64,
    epochs=5,
    verbose=1,
    shuffle=True
)
# Epoch (ì—í­): ì „ì²´ í•™ìŠµ ë°ì´í„°ë¥¼ í•œ ë²ˆ ë‹¤ í•™ìŠµí•œ íšŸìˆ˜ì•¼. ë„ˆëŠ” 5ë²ˆ ëŒë ¸ì–´.
# Loss (ë¡œìŠ¤): ëª¨ë¸ì´ í‹€ë¦° ì •ë„ì•¼. ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ.
# Accuracy (ì •í™•ë„): ëª¨ë¸ì´ ë§ì¶˜ ë¹„ìœ¨ì´ì•¼. í´ìˆ˜ë¡ ì¢‹ìŒ.
# ëª¨ë¸ì„ í›ˆë ¨í•  ë•Œ, ì „ì²´ ë°ì´í„°ë¥¼ í›ˆë ¨ìš©(train) ê³¼ ê²€ì¦ìš©(validation) ìœ¼ë¡œ ë‚˜ëˆ .
# í›ˆë ¨ìš© ë°ì´í„°ëŠ” ëª¨ë¸ì´ ì§ì ‘ ë³´ê³  í•™ìŠµí•˜ëŠ” ë°ì´í„°ì•¼.
# ê²€ì¦ìš© ë°ì´í„°(val) ëŠ” ëª¨ë¸ì´ ì§ì ‘ ë³´ì§€ ì•Šì€ ë°ì´í„°ë¡œ, ëª¨ë¸ì´ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•˜ëŠ” ë° ì¨.
# val_ë¡œ ì‹œì‘í•˜ëŠ” ê±´ ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ì´ì•¼ (ëª¨ë¸ì´ ì‹¤ì œë¡œ ì–¼ë§ˆë‚˜ ì˜ ì¼ë°˜í™”ë˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ìš©ë„).
# val_loss: ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì†ì‹¤ê°’
# val_accuracy: ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì •í™•ë„
# í›ˆë ¨ ì •í™•ë„ë§Œ ë³´ë©´ ê³¼ì í•©(overfitting)ì´ ì¼ì–´ë‚¬ëŠ”ì§€ ì•Œ ìˆ˜ ì—†ì–´.
# ê²€ì¦ ì •í™•ë„ê°€ ë†’ì•„ì•¼ ì§„ì§œ "ì‹¤ì „ì—ì„œë„ ì˜ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸"ì´ë¼ê³  í•  ìˆ˜ ìˆì–´. 
# âœ… loss = ì†ì‹¤(ì˜¤ì°¨ì˜ ì •ë„)
# ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’ê³¼ ì‹¤ì œ ì •ë‹µ ì‚¬ì´ì˜ ì°¨ì´(ì˜¤ì°¨) ë¥¼ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•œ ê±°ì•¼.
# ì˜ˆë¥¼ ë“¤ì–´ ìˆ«ì 7ì„ ì´ë¯¸ì§€ë¡œ ë„£ì—ˆëŠ”ë°, ëª¨ë¸ì´ 5ë¼ê³  ì˜ˆì¸¡í•˜ë©´ ì°¨ì´ê°€ ìˆìœ¼ë‹ˆê¹Œ lossê°€ ì»¤ì ¸.
# ëª¨ë¸ì€ lossë¥¼ ì¤„ì´ê¸° ìœ„í•´ í•™ìŠµí•´.
# ğŸ“‰ ë‚®ì„ìˆ˜ë¡ ì¢‹ë‹¤!
# lossê°€ ë‚®ì•„ì§„ë‹¤ëŠ” ê±´ ëª¨ë¸ì´ ì •ë‹µì— ê°€ê¹Œì›Œì§€ê³  ìˆë‹¤ëŠ” ì˜ë¯¸ì•¼.
# í•˜ì§€ë§Œ lossê°€ ë‚®ë‹¤ê³  ì •í™•ë„ê°€ ë°˜ë“œì‹œ ë†’ì€ ê±´ ì•„ëƒ, íŠ¹íˆ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì¼ ë•ŒëŠ” ì •í™•ë„ì™€ í•¨ê»˜ ê°™ì´ ë´ì•¼ í•´.   
# ________í•™ìŠµê³¼ì • ì¶œë ¥_________________________________________________________
# Epoch 1/5                                                               í›ˆë ¨ì •í™•ë„ 90.01%                          ê²€ì¦ì •í™•ë„ 93.96%
# 938/938 [==============================] - 5s 4ms/step - loss: 0.3579 - accuracy: 0.9001 - val_loss: 0.2031 - val_accuracy: 0.9396
# Epoch 2/5
# 938/938 [==============================] - 3s 3ms/step - loss: 0.1819 - accuracy: 0.9485 - val_loss: 0.1528 - val_accuracy: 0.9532
# Epoch 3/5
# 938/938 [==============================] - 3s 3ms/step - loss: 0.1385 - accuracy: 0.9596 - val_loss: 0.1285 - val_accuracy: 0.9637
# Epoch 4/5
# 938/938 [==============================] - 3s 3ms/step - loss: 0.1119 - accuracy: 0.9672 - val_loss: 0.1125 - val_accuracy: 0.9657
# Epoch 5/5
# 938/938 [==============================] - 3s 3ms/step - loss: 0.0939 - accuracy: 0.9719 - val_loss: 0.1053 - val_accuracy: 0.9679
# â€¢ ê³¼ì í•© ì—†ìŒ: trainê³¼ val ì •í™•ë„ ì°¨ì´ ê±°ì˜ ì—†ìŒ.
# â€¢ ì ì§„ì ì¸ ì„±ëŠ¥ í–¥ìƒ: ì •í™•íˆ ì˜ í•™ìŠµë˜ê³  ìˆìŒ.
# â€¢ 5 epochë§Œì— 96.8% â†’ êµ‰ì¥íˆ ìš°ìˆ˜í•œ ì„±ëŠ¥ (MNIST ê¸°ì¤€).
# ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ë¥¼ ì ì  ë” ì˜ ë§ì¶”ê³  ìˆê³ ,
# ê²€ì¦ ë°ì´í„°ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆì–´ì„œ,
# ê³¼ì í•© ì—†ì´ ì•„ì£¼ ì˜ í›ˆë ¨ë˜ê³  ìˆë‹¤ëŠ” ëœ»ì´ì•¼.



# model = keras.Sequential([
# #2-1.ì…ë ¥ì¸µ ì„¤ê³„
# #layers.Dense(ì¶œë ¥ê°’ì˜ê°œìˆ˜, í™œì„±í™”í•¨ìˆ˜, ì…ë ¥ë°ì´í„°ì˜ í¬ê¸°-ìƒëµê°€ëŠ¥)
# #ì¶œë ¥ê°’ê°œìˆ˜? ì € ê³„ì¸µì„ ë‚˜ì™”ì„ë•Œ ê°€ì ¸ì˜¬ ê°€ì¤‘ì¹˜ë“¤ì˜ ê°œìˆ˜.
# #           ë‚´ë§ˆìŒëŒ€ë¡œ ë„ˆë¬´ í¬ê²Œ ì£¼ë©´, ë©”ëª¨ë¦¬ë¶€ì¡±ê³¼ ê³¼ëŒ€ì í•© ë¬¸ì œ,
# #           ì ë‹¹íˆ 2ì˜ ë°°ìˆ˜ë¡œë“¤ ì¤€ë‹¤
#     layers.Dense(64, activation='relu' ),
# #2-2 ì¤‘ê°„ì— ë‹¤ë¥¸ì¸µ ì¶”ê°€ ê°€ëŠ¥
# #2-3 ì¶œë ¥ì¸µ,ë§ˆì§€ë§‰ì¸µì€ ë¼ë²¨ì— ë§ì¶”ê¸°.ê²°ê³¼ì–»ê¸°ìœ„í•œì¸µ
# #    ì†ìœ¼ë¡œì“´ ìˆ«ìì´ë‹ˆ 0~9 ì¤‘ì— í•˜ë‚˜ì—¬ì•¼í•œë‹¤.
# #   ë”¥ëŸ¬ë‹ë¶„ë¥˜ëŠ” ì¶œë ¥ë°ì´í„°ë¥¼ í™•ë¥ ë¡œ ë°˜í™˜
# #   ì˜ˆ)[0.1,0.1,0.05,0.7...] ê²°ê³¼ëŠ”3ìœ¼ë¡œ íŒë‹¨
# #   ê°ì¸µ ê±°ì¹˜ë©° ë‚˜ì˜¤ëŠ”ê°’ë“¤ì€ ì‹¤ì œí™•ë¥ ì´ ì•„ë‹Œ ì—„ì²­í°ê°’ë“¤.
# #   ì´ë¥¼ ëª¨ë‘í•©í•´ 1ì´ë˜ëŠ” í™•ë¥ ë¡œ ì „í™˜í•´ì•¼í•˜ëŠ”ë° ì´ í•¨ìˆ˜ê°€ softmax í•¨ìˆ˜
# #   ë‹¤ì¤‘ë¶„ë¥˜ì˜ ì¶œë ¥ì¸µì˜ í™œì„±í™”í•¨ìˆ˜ëŠ” ë¬´ì¡°ê±´ softmaxí•¨ìˆ˜
# #ë‚´ìš© ì™¸ì›Œì§ˆë•Œê¹Œì§€ ì—°ìŠµí•´ì•¼í•¨
#     layers.Dense(10, activation='softmax') #ì¶œë ¥ê°’ê°œìˆ˜, í™œì„±í™”í•¨ìˆ˜
# #íšŒê·€ë‘ ì´ì§„ë¶„ë¥˜,ë‹¤ì¤‘ë¶„ë¥˜ ë‹¤ ë‹¤ë¥´ê²Œ ì‘ì„±.
# #íšŒê·€ëŠ” ì¶œë ¥ê²°ê³¼1ê°œë§Œ
# #íšŒê·€ì˜ê²½ìš° ì¶œë ¥ì¸µ: layers.Dense(1, activation='linear')
# #ì´ì§„ë¶„ë¥˜ì˜ê²½ìš° ì¶œë ¥ì¸µ: layers.Dense(1, activation='sigmoid')
# ])

# #compile() ë©”ì„œë“œë¡œ ëª¨ë¸ì„ ì»´íŒŒì¼í•œë‹¤.(ëª¨ë¸í•™ìŠµì‹œí‚¬ì¤€ë¹„í•œë‹¤)
# # ëª¨ë¸ì„ ì»´íŒŒì¼í•  ë•ŒëŠ” ë‹¤ìŒê°™ì€ ë§¤ê°œë³€ìˆ˜ ì§€ì •
# #loss: ì†ì‹¤í•¨ìˆ˜, ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’ê³¼ ì‹¤ì œ ê°’ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
# #       ë‹¤ì¤‘ë¶„ë¥˜ëŠ” sparse_categorical_crossentropy, ì´ì§„ë¶„ë¥˜ëŠ” binary_crossentropy
# #       íšŒê·€ëŠ” mean_squared_error
# #optimizer: ìµœì í™” ì•Œê³ ë¦¬ì¦˜, adam, sgd ë“±
# #metrics: ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ë•Œ ì‚¬ìš©í•  í‰ê°€ ì§€í‘œ, accuracy(ì •í™•ë„) ë“±
# #       ì´ì§„ë¶„ë¥˜ëŠ” accuracy, ë‹¤ì¤‘ë¶„ë¥˜ëŠ” sparse_categorical_accuracy

# #3.ë°ì´í„°ê´€ë ¨ì‘ì—…(3D â†’ 2D ë³€í™˜, ì •ê·œí™” ìŠ¤ì¼€ì¼ë§:ë”¥ëŸ¬ë‹ì€í•„ìˆ˜)
# # reshape() ë©”ì„œë“œë¡œ 3ì°¨ì› ë°°ì—´ì„ 2ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
# # MNIST ë°ì´í„°ëŠ” 28x28 í¬ê¸°ì˜ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
# # train_images = train_images.reshape((60000, 28 * 28))
# # test_images = test_images.reshape((10000, 28 * 28))
# train_images = train_images.reshape(train_images.shape[0], 28 * 28)
# train_images = train_images.astype(float) / 255 # ì •ê·œí™”: 0~255 â†’ 0~1ë¡œ ë³€í™˜
# test_images = test_images.reshape(test_images.shape[0], 28 * 28)
# test_images = test_images.astype(float) / 255

# # loss = 'sparse_categorical_crossentropy' # ì†ì‹¤í•¨ìˆ˜ì•ˆì“°ê³  categorical_crossentropyë¡œ í•˜ë©´
# # ì›í•«ì¸ì½”ë”©ëœ ë¼ë²¨ì´ í•„ìš”í•˜ë‹¤. ì¦‰, y = [[1,0,0], [0,1,0], ...] í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì•¼í•¨
# # sparse_categorical_crossentropyë¥¼ ì‚¬ìš©í•˜ë©´ ë¼ë²¨ì„ ìˆ«ìí˜•ìœ¼ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜

# #4.ëª¨ë¸ í•™ìŠµ: í•™ìŠµê³¼ì •ë‚´ìš© history ê°ì²´ë¡œ ë°˜í™˜
# # train_images: í›ˆë ¨ ë°ì´í„°, train_labels: í›ˆë ¨ ë°ì´í„°ì˜ ë¼ë²¨
# #fit() ë©”ì„œë“œë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤.
# history = model.fit(train_images,   #í›ˆë ¨ ë°ì´í„°:X,ë…ë¦½ë³€ìˆ˜,ì…ë ¥ê°’
#                     train_labels,   #í›ˆë ¨ ë°ì´í„°ì˜ ë¼ë²¨:y, ì¢…ì†ë³€ìˆ˜, ì¶œë ¥ê°’
#                     validation_data=(test_images, test_labels), #ê²€ì¦ìš© ë°ì´í„°
#                     batch_size=64, # ë°°ì¹˜ì‚¬ì´ì¦ˆ: í•œë²ˆì— ëª‡ê°œì”© í•™ìŠµí• ì§€
#                     # ë°ì´í„° ë©”ëª¨ë¦¬ ë¶ˆëŸ¬ì˜¬ë•Œ í¬ê¸°ë„ˆë¬´í¬ë©´ ë©”ëª¨ë¦¬ë¶€ì¡±, ë„ˆë¬´ì‘ìœ¼ë©´ í•™ìŠµì‹œê°„ ì˜¤ë˜ê±¸ë¦¼(ì†ë„ëŠë¦¼)
#                     # ì „ì²´ ë°ì´í„°ë¥¼ 64ê°œì”© ë‚˜ëˆ ì„œ í•™ìŠµ. batch_size=64ë§Œí¼ ë¶ˆëŸ¬ì„œ í•™ìŠµëë‚œ1ë°”í€´ê°€ 1ì—í¬í¬
#                     # epochs=5ë©´ 64ê°œì”© 5ë²ˆ í•™ìŠµ
#                     epochs=5, # ì—í­: ì „ì²´ ë°ì´í„°ì…‹ì„ ëª‡ë²ˆ ë°˜ë³µí•´ì„œ í•™ìŠµí• ì§€ í•™ìŠµíšŒìˆ˜, epochs=5ë©´ 64ê°œì”© 5ë²ˆ í•™ìŠµ
#                     verbose=1, # í•™ìŠµê³¼ì • ì¶œë ¥ì—¬ë¶€, 0:ì¶œë ¥ì•ˆí•¨, 1:ì§„í–‰ìƒí™©ì¶œë ¥, 2: ì—í­ë³„ë¡œì¶œë ¥
#                     shuffle=True, # ì—í­ë§ˆë‹¤ ë°ì´í„° ì„ê¸° ì—¬ë¶€, True: ì„ìŒ
#                     # callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)], # ì¡°ê¸° ì¢…ë£Œ ì½œë°±
#                     # callbacks=[keras.callbacks.ModelCheckpoint(filepath='model.h5', save_best_only=True)],
#                     validation_split=0.2) # ê²€ì¦ìš© ë°ì´í„°ë¹„ìœ¨20%

# # ë¨¸ì‹ ëŸ¬ë‹ vs ë”¥ëŸ¬ë‹ ë¼ë²¨ ì¸ì½”ë”© ì •ë¦¬ ë©”ëª¨
# # ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ (ëœë¤í¬ë ˆìŠ¤íŠ¸, SVM ë“±)
# # â†’ ìˆ«ìí˜• ë¼ë²¨ ì‚¬ìš© (ë¼ë²¨ ì¸ì½”ë”©)
# # â†’ y = [0, 1, 2, ...] í˜•ì‹ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥
# # ìŒ¤:ë¨¸ì‹ ëŸ¬ë‹ì€ ë¼ë²¨ì„ ì›í•«ì¸ì½”ë”©, ë”¥ëŸ¬ë‹ì€ ì›í•«ì¸ì½”ë”©ìë™.
# # ë”¥ëŸ¬ë‹ ëª¨ë¸ (Keras, TensorFlow ë“±)
# # loss = 'categorical_crossentropy' â†’ ì›í•«ì¸ì½”ë”© í•„ìš”: y = [[1,0,0], [0,1,0], ...]
# # loss = 'sparse_categorical_crossentropy' â†’ ìˆ«ìí˜• ë¼ë²¨ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥
# # ë”¥ëŸ¬ë‹ì´ ë¼ë²¨ì„ ë‚´ë¶€ì ìœ¼ë¡œ ìë™ ì›í•« ì¸ì½”ë”©í•˜ì§€ëŠ” ì•ŠìŒ â†’ ì¸ì½”ë”©ì€ ì‚¬ìš©ìê°€ ì§ì ‘ ì²˜ë¦¬í•˜ê±°ë‚˜ ì†ì‹¤ í•¨ìˆ˜ë¡œ ì œì–´
