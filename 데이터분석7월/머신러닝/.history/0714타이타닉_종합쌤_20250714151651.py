import pandas as pd
import numpy as np
import os #íŒŒì¼ì´ë‚˜ í´ë”ê²½ë¡œ ì§€ì •ì‹œ í•„ìš”

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# -------------------------------------------------
# âœ” STEP 1  â”€â”€ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° & í•„ìš” ì—†ëŠ” ì—´ ì œê±°
# -------------------------------------------------
# path = r"C:\Users\Admin\Documents\GitHub\python_workspace1N\ë°ì´í„°ë¶„ì„7ì›”\data"
train = pd.read_csv("./data/titanic_train.csv")
test  = pd.read_csv("./data/titanic_test.csv")

#1.ë¶ˆí•„ìš”í•œì—´ì‚­ì œ
print(train.head()) #ì›ë³¸ë°ì´í„° inplace=Trueì•ˆë¨¹ížˆëŠ”í•¨ìˆ˜ë§Žì•„ ë°˜í™˜ê°’ë°›ê³  shpeì°ì–´ì„œ í™•ì¸
DROP_COLS = ["PassengerId", "Name", "Ticket", "SibSp", "Parch"]
train.drop(columns=DROP_COLS, inplace=True)
test.drop(columns=[c for c in DROP_COLS if c in test.columns], inplace=True)  # testì—” Survived ì—†ìŒ
print(train.head())
print(train.head())
# -------------------------------------------------
# âœ” STEP 2  â”€â”€ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# -------------------------------------------------
for df in (train, test):
    df["Age"].fillna(df["Age"].mean(), inplace=True)           # ì—°ì†í˜• â†’ í‰ê· 
    df["Fare"].fillna(df["Fare"].median(), inplace=True)       # ì—°ì†í˜• â†’ ì¤‘ì•™ê°’
    df["Embarked"].fillna(df["Embarked"].mode()[0], inplace=True)  # ë²”ì£¼í˜• â†’ ìµœë¹ˆê°’
    df["Cabin"] = df["Cabin"].fillna("N").str[0]               # ê²°ì¸¡ â†’ 'N', ì•žê¸€ìžë§Œ

# -------------------------------------------------
# âœ” STEP 3  â”€â”€ ì´ìƒì¹˜ ì œê±° (IQR 1.5ë°° ë°”ê¹¥ê°’)
# -------------------------------------------------
# def remove_outliers(df, col_list):
#     for col in col_list:
#         Q1 = df[col].quantile(0.25)
#         Q3 = df[col].quantile(0.75)
#         IQR = Q3 - Q1
#         low  = Q1 - 1.5 * IQR
#         high = Q3 + 1.5 * IQR
#         df = df[(df[col] >= low) & (df[col] <= high)]
#     return df.reset_index(drop=True)

# train = remove_outliers(train, ["Age", "Fare"])

# # -------------------------------------------------
# # âœ” STEP 4  â”€â”€ ì¤‘ë³µ í–‰ ì œê±°
# # -------------------------------------------------
# train.drop_duplicates(inplace=True)

# # -------------------------------------------------
# # âœ” STEP 5  â”€â”€ ê°’ ì˜¤ë¥˜ ìˆ˜ì • (ë²”ì£¼í˜• ê°’ ì´ìƒ ì—¬ë¶€ ê²€í† )
# # -------------------------------------------------
# # Sexì™€ Embarkedì˜ ê³ ìœ ê°’ì´ ì˜ˆìƒ ë²”ì£¼ì™€ ë‹¤ë¥´ë©´ ì°ì–´ë³´ê¸°
# for col in ["Sex", "Embarked", "Cabin"]:
#     bad = train[~train[col].isin(train[col].unique())]
#     if not bad.empty:
#         print(f"[WARNING] {col} ì´ìƒê°’:\n", bad[col].value_counts())

# # -------------------------------------------------
# # âœ” STEP 6  â”€â”€ ë¼ë²¨/ì›â€‘í•« ì¸ì½”ë”©
# # -------------------------------------------------
# # Cabin(ì•žê¸€ìž), Embarked, Pclass(ë²”ì£¼í˜•)ì€ ì›â€‘í•« / SexëŠ” 0â€‘1 ë§¤í•‘ìœ¼ë¡œ ì²˜ë¦¬
# CATEGORICAL = ["Cabin", "Embarked", "Pclass"]
# BINARY       = ["Sex"]
# NUMERICAL    = ["Age", "Fare"]

# # Sexë¥¼ 0/1ë¡œ
# train["Sex"] = train["Sex"].map({"male": 0, "female": 1})
# test["Sex"]  = test["Sex"].map({"male": 0, "female": 1})

# # -------------------------------------------------
# # âœ” STEP 7  â”€â”€ ìŠ¤ì¼€ì¼ë§ + ëª¨ë¸ íŒŒì´í”„ë¼ì¸
# # -------------------------------------------------
# target = train["Survived"]
# features = train.drop("Survived", axis=1)

# X_train, X_val, y_train, y_val = train_test_split(
#     features, target, test_size=0.2, random_state=42, stratify=target
# )

# # ì»¬ëŸ¼ ë³€í™˜ê¸°
# preprocess = ColumnTransformer(
#     transformers=[
#         ("num", StandardScaler(), NUMERICAL),
#         ("cat", "passthrough",  CATEGORICAL),   # ì›â€‘í•«ì€ get_dummiesë¡œ ë¯¸ë¦¬ í•˜ì§€ ì•Šê³  RFë¼ì„œ ê·¸ëŒ€ë¡œ ë‘¬ë„ ok
#         ("bin", "passthrough",  BINARY)
#     ],
#     remainder="drop"
# )

# rf = RandomForestClassifier(random_state=42)

# pipe = Pipeline(steps=[
#     ("prep", preprocess),
#     ("model", rf)
# ])

# param_grid = {
#     "model__n_estimators": [50, 100, 200],
#     "model__max_depth":    [None, 5, 10],
#     "model__min_samples_split": [2, 5]
# }

# grid = GridSearchCV(pipe, param_grid, cv=5, scoring="accuracy", verbose=1)
# grid.fit(X_train, y_train)

# # -------------------------------------------------
# # âœ” STEP 8  â”€â”€ ì„±ëŠ¥ í‰ê°€ & íŠ¹ì„± ì¤‘ìš”ë„
# # -------------------------------------------------
# best_pipe = grid.best_estimator_
# y_pred = best_pipe.predict(X_val)

# print("ðŸ“Š ìµœì  íŒŒë¼ë¯¸í„°:", grid.best_params_)
# print("âœ… ê²€ì¦ ì •í™•ë„:", accuracy_score(y_val, y_pred))
# print("\nðŸ“„ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n", classification_report(y_val, y_pred))

# # íŠ¹ì„± ì¤‘ìš”ë„ (íŒŒì´í”„ë¼ì¸ ë‚´ë¶€ì—ì„œ RFê°€ í•™ìŠµëìœ¼ë‹ˆ ê°€ì ¸ì™€ì„œ ì¶œë ¥)
# rf_model = best_pipe.named_steps["model"]
# feature_names = (
#     NUMERICAL                                         # ìŠ¤ì¼€ì¼ë§ëœ ìˆ˜ì¹˜
#     + CATEGORICAL                                     # ê·¸ëŒ€ë¡œ í†µê³¼
#     + BINARY                                          # Sex
# )
# importances = pd.Series(rf_model.feature_importances_, index=feature_names)
# importances.sort_values(ascending=False, inplace=True)

# print("\nðŸ” Feature Importances")
# print(importances.round(3))
